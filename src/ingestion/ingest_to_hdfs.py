import sys
import argparse
from pyspark.sql import SparkSession
from pyspark.sql.functions import max, lit
from pyspark.sql.types import IntegerType

def get_spark_session(app_name):
    """
    T·∫°o Spark Session v·ªõi c·∫•u h√¨nh h·ªó tr·ª£ Hive v√† MySQL JDBC
    """
    spark = SparkSession.builder \
        .appName(app_name) \
        .config("spark.hadoop.fs.defaultFS", "hdfs://namenode:9000") \
        .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
        .config("hive.metastore.uris", "thrift://hive-metastore:9083") \
        .enableHiveSupport() \
        .getOrCreate()
    return spark

def get_max_id_from_datalake(spark, hdfs_path):
    """
    Ki·ªÉm tra xem trong HDFS ƒë√£ c√≥ d·ªØ li·ªáu ch∆∞a.
    N·∫øu c√≥ -> Tr·∫£ v·ªÅ ID l·ªõn nh·∫•t (Max ID) ƒë·ªÉ l√†m m·ªëc t·∫£i ti·∫øp.
    N·∫øu ch∆∞a -> Tr·∫£ v·ªÅ 0 (T·∫£i t·ª´ ƒë·∫ßu).
    """
    try:
        # Th·ª≠ ƒë·ªçc d·ªØ li·ªáu t·ª´ HDFS
        # Ch√∫ √Ω: Spark s·∫Ω b√°o l·ªói n·∫øu ƒë∆∞·ªùng d·∫´n kh√¥ng t·ªìn t·∫°i, n√™n c·∫ßn try/except
        df_lake = spark.read.parquet(hdfs_path)
        
        # N·∫øu ƒë·ªçc ƒë∆∞·ª£c, t√¨m max id
        max_row = df_lake.agg(max("id")).collect()[0]
        max_id = max_row[0]
        
        if max_id is None:
            return 0
        return max_id
        
    except Exception as e:
        # N·∫øu l·ªói (th∆∞·ªùng l√† do path ch∆∞a t·ªìn t·∫°i), coi nh∆∞ ch∆∞a c√≥ d·ªØ li·ªáu
        print(f"‚ö†Ô∏è Chua co du lieu tai {hdfs_path}. Se tai tu dau (Full Load).")
        return 0

def main():
    # 1. Nh·∫≠n tham s·ªë ƒë·∫ßu v√†o (gi·ªëng args c·ªßa anh C·∫£nh)
    parser = argparse.ArgumentParser(description='Ingest MySQL to HDFS')
    parser.add_argument('--tblName', required=True, help='Ten bang trong MySQL')
    parser.add_argument('--executionDate', required=True, help='Ngay chay (YYYY-MM-DD)')
    args = parser.parse_args()

    table_name = args.tblName
    execution_date = args.executionDate
    
    # T√°ch ng√†y th√°ng nƒÉm ƒë·ªÉ l√†m Partition
    y, m, d = execution_date.split('-')

    print(f"üöÄ BAT DAU INGESTION: Table={table_name} | Date={execution_date}")

    # 2. Kh·ªüi t·∫°o Spark
    spark = get_spark_session(f"Ingest {table_name}")

    # C·∫•u h√¨nh HDFS Path (Data Lake)
    hdfs_path = f"hdfs://namenode:9000/datalake/{table_name}"

    # 3. Logic Incremental Load (Ki·ªÉm tra d·ªØ li·ªáu c≈©)
    # L∆∞u √Ω: Logic n√†y √°p d·ª•ng cho b·∫£ng c√≥ c·ªôt 'id' tƒÉng d·∫ßn (nh∆∞ travel, music, movie buzz)
    # ƒê·ªëi v·ªõi b·∫£ng metadata (√≠t thay ƒë·ªïi), ta c√≥ th·ªÉ ch·ªçn c√°ch ghi ƒë√® (overwrite)
    
    # ·ªû ƒë√¢y demo logic Incremental cho b·∫£ng Buzz/Review
    max_id = 0
    # Ch·ªâ √°p d·ª•ng check max_id cho c√°c b·∫£ng d·ªØ li·ªáu l·ªõn, b·∫£ng metadata th√¨ th√¥i
    if "buzz" in table_name or "data" in table_name: 
        # Tuy nhi√™n trong ng·ªØ c·∫£nh demo l·∫ßn ƒë·∫ßu, HDFS ch∆∞a c√≥ g√¨ n√™n max_id lu√¥n = 0
        # ƒê·ªÉ ƒë∆°n gi·∫£n cho l·∫ßn ch·∫°y ƒë·∫ßu, ta c·ª© set query l·∫•y h·∫øt.
        pass 
    
    # Query l·∫•y d·ªØ li·ªáu (Logic c·ªßa anh C·∫£nh: SELECT * FROM table WHERE id > max_id)
    # L∆∞u √Ω: id trong file csv l√† UUID (chu·ªói), kh√¥ng so s√°nh l·ªõn nh·ªè nh∆∞ s·ªë ƒë∆∞·ª£c.
    # ƒê·ªÉ demo ƒë∆°n gi·∫£n, ta s·∫Ω load to√†n b·ªô theo ng√†y (ho·∫∑c load h·∫øt n·∫øu l·∫ßn ƒë·∫ßu).
    
    # Trong th·ª±c t·∫ø production v·ªõi UUID, ng∆∞·ªùi ta th∆∞·ªùng d√πng c·ªôt 'created_at' ho·∫∑c 'updated_at'
    # ·ªû ƒë√¢y m√¨nh s·∫Ω load full b·∫£ng t·ª´ MySQL (v√¨ dataset 10k d√≤ng c√≤n nh·ªè v·ªõi Spark)
    print("‚è≥ Dang doc du lieu tu MySQL...")
    
    jdbc_url = "jdbc:mysql://mysql:3306/social_trend_db?useSSL=false&allowPublicKeyRetrieval=true"
    
    df_source = spark.read.format("jdbc") \
        .option("url", jdbc_url) \
        .option("dbtable", table_name) \
        .option("user", "user") \
        .option("password", "password") \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .load()

    print(f"üìä Da doc duoc {df_source.count()} dong tu MySQL.")

    # 4. Th√™m c·ªôt Partition (Year, Month, Day)
    # Gi·∫£ l·∫≠p: G√°n partition l√† ng√†y ch·∫°y job
    df_final = df_source \
        .withColumn("year", lit(y)) \
        .withColumn("month", lit(m)) \
        .withColumn("day", lit(d))

    # 5. Ghi xu·ªëng HDFS (Data Lake)
    print(f"üíæ Dang ghi xuong HDFS tai: {hdfs_path} ...")
    
    df_final.write \
        .mode("append") \
        .partitionBy("year", "month", "day") \
        .parquet(hdfs_path)

    print("‚úÖ INGESTION HOAN TAT!")
    spark.stop()

if __name__ == "__main__":
    main()